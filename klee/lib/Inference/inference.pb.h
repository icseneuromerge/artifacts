// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: inference.proto

#ifndef GOOGLE_PROTOBUF_INCLUDED_inference_2eproto
#define GOOGLE_PROTOBUF_INCLUDED_inference_2eproto

#include <limits>
#include <string>

#include <google/protobuf/port_def.inc>
#if PROTOBUF_VERSION < 3018000
#error This file was generated by a newer version of protoc which is
#error incompatible with your Protocol Buffer headers. Please update
#error your headers.
#endif
#if 3018001 < PROTOBUF_MIN_PROTOC_VERSION
#error This file was generated by an older version of protoc which is
#error incompatible with your Protocol Buffer headers. Please
#error regenerate this file with a newer version of protoc.
#endif

#include <google/protobuf/port_undef.inc>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/arena.h>
#include <google/protobuf/arenastring.h>
#include <google/protobuf/generated_message_table_driven.h>
#include <google/protobuf/generated_message_util.h>
#include <google/protobuf/metadata_lite.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/message.h>
#include <google/protobuf/repeated_field.h>  // IWYU pragma: export
#include <google/protobuf/extension_set.h>  // IWYU pragma: export
#include <google/protobuf/map.h>  // IWYU pragma: export
#include <google/protobuf/map_entry.h>
#include <google/protobuf/map_field_inl.h>
#include <google/protobuf/unknown_field_set.h>
#include <google/protobuf/empty.pb.h>
// @@protoc_insertion_point(includes)
#include <google/protobuf/port_def.inc>
#define PROTOBUF_INTERNAL_EXPORT_inference_2eproto
PROTOBUF_NAMESPACE_OPEN
namespace internal {
class AnyMetadata;
}  // namespace internal
PROTOBUF_NAMESPACE_CLOSE

// Internal implementation detail -- do not use these members.
struct TableStruct_inference_2eproto {
  static const ::PROTOBUF_NAMESPACE_ID::internal::ParseTableField entries[]
    PROTOBUF_SECTION_VARIABLE(protodesc_cold);
  static const ::PROTOBUF_NAMESPACE_ID::internal::AuxiliaryParseTableField aux[]
    PROTOBUF_SECTION_VARIABLE(protodesc_cold);
  static const ::PROTOBUF_NAMESPACE_ID::internal::ParseTable schema[4]
    PROTOBUF_SECTION_VARIABLE(protodesc_cold);
  static const ::PROTOBUF_NAMESPACE_ID::internal::FieldMetadata field_metadata[];
  static const ::PROTOBUF_NAMESPACE_ID::internal::SerializationTable serialization_table[];
  static const ::PROTOBUF_NAMESPACE_ID::uint32 offsets[];
};
extern const ::PROTOBUF_NAMESPACE_ID::internal::DescriptorTable descriptor_table_inference_2eproto;
namespace org {
namespace pytorch {
namespace serve {
namespace grpc {
namespace inference {
class PredictionResponse;
struct PredictionResponseDefaultTypeInternal;
extern PredictionResponseDefaultTypeInternal _PredictionResponse_default_instance_;
class PredictionsRequest;
struct PredictionsRequestDefaultTypeInternal;
extern PredictionsRequestDefaultTypeInternal _PredictionsRequest_default_instance_;
class PredictionsRequest_InputEntry_DoNotUse;
struct PredictionsRequest_InputEntry_DoNotUseDefaultTypeInternal;
extern PredictionsRequest_InputEntry_DoNotUseDefaultTypeInternal _PredictionsRequest_InputEntry_DoNotUse_default_instance_;
class TorchServeHealthResponse;
struct TorchServeHealthResponseDefaultTypeInternal;
extern TorchServeHealthResponseDefaultTypeInternal _TorchServeHealthResponse_default_instance_;
}  // namespace inference
}  // namespace grpc
}  // namespace serve
}  // namespace pytorch
}  // namespace org
PROTOBUF_NAMESPACE_OPEN
template<> ::org::pytorch::serve::grpc::inference::PredictionResponse* Arena::CreateMaybeMessage<::org::pytorch::serve::grpc::inference::PredictionResponse>(Arena*);
template<> ::org::pytorch::serve::grpc::inference::PredictionsRequest* Arena::CreateMaybeMessage<::org::pytorch::serve::grpc::inference::PredictionsRequest>(Arena*);
template<> ::org::pytorch::serve::grpc::inference::PredictionsRequest_InputEntry_DoNotUse* Arena::CreateMaybeMessage<::org::pytorch::serve::grpc::inference::PredictionsRequest_InputEntry_DoNotUse>(Arena*);
template<> ::org::pytorch::serve::grpc::inference::TorchServeHealthResponse* Arena::CreateMaybeMessage<::org::pytorch::serve::grpc::inference::TorchServeHealthResponse>(Arena*);
PROTOBUF_NAMESPACE_CLOSE
namespace org {
namespace pytorch {
namespace serve {
namespace grpc {
namespace inference {

// ===================================================================

class PredictionsRequest_InputEntry_DoNotUse : public ::PROTOBUF_NAMESPACE_ID::internal::MapEntry<PredictionsRequest_InputEntry_DoNotUse, 
    std::string, std::string,
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_STRING,
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_BYTES> {
public:
  typedef ::PROTOBUF_NAMESPACE_ID::internal::MapEntry<PredictionsRequest_InputEntry_DoNotUse, 
    std::string, std::string,
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_STRING,
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_BYTES> SuperType;
  PredictionsRequest_InputEntry_DoNotUse();
  explicit constexpr PredictionsRequest_InputEntry_DoNotUse(
      ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);
  explicit PredictionsRequest_InputEntry_DoNotUse(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  void MergeFrom(const PredictionsRequest_InputEntry_DoNotUse& other);
  static const PredictionsRequest_InputEntry_DoNotUse* internal_default_instance() { return reinterpret_cast<const PredictionsRequest_InputEntry_DoNotUse*>(&_PredictionsRequest_InputEntry_DoNotUse_default_instance_); }
  static bool ValidateKey(std::string* s) {
    return ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(s->data(), static_cast<int>(s->size()), ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::PARSE, "org.pytorch.serve.grpc.inference.PredictionsRequest.InputEntry.key");
 }
  static bool ValidateValue(void*) { return true; }
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;
};

// -------------------------------------------------------------------

class PredictionsRequest final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:org.pytorch.serve.grpc.inference.PredictionsRequest) */ {
 public:
  inline PredictionsRequest() : PredictionsRequest(nullptr) {}
  ~PredictionsRequest() override;
  explicit constexpr PredictionsRequest(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  PredictionsRequest(const PredictionsRequest& from);
  PredictionsRequest(PredictionsRequest&& from) noexcept
    : PredictionsRequest() {
    *this = ::std::move(from);
  }

  inline PredictionsRequest& operator=(const PredictionsRequest& from) {
    CopyFrom(from);
    return *this;
  }
  inline PredictionsRequest& operator=(PredictionsRequest&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const PredictionsRequest& default_instance() {
    return *internal_default_instance();
  }
  static inline const PredictionsRequest* internal_default_instance() {
    return reinterpret_cast<const PredictionsRequest*>(
               &_PredictionsRequest_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    1;

  friend void swap(PredictionsRequest& a, PredictionsRequest& b) {
    a.Swap(&b);
  }
  inline void Swap(PredictionsRequest* other) {
    if (other == this) return;
    if (GetOwningArena() == other->GetOwningArena()) {
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(PredictionsRequest* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  inline PredictionsRequest* New() const final {
    return new PredictionsRequest();
  }

  PredictionsRequest* New(::PROTOBUF_NAMESPACE_ID::Arena* arena) const final {
    return CreateMaybeMessage<PredictionsRequest>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const PredictionsRequest& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom(const PredictionsRequest& from);
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to, const ::PROTOBUF_NAMESPACE_ID::Message& from);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  ::PROTOBUF_NAMESPACE_ID::uint8* _InternalSerialize(
      ::PROTOBUF_NAMESPACE_ID::uint8* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(PredictionsRequest* other);
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "org.pytorch.serve.grpc.inference.PredictionsRequest";
  }
  protected:
  explicit PredictionsRequest(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------


  // accessors -------------------------------------------------------

  enum : int {
    kInputFieldNumber = 3,
    kModelNameFieldNumber = 1,
    kModelVersionFieldNumber = 2,
  };
  // map<string, bytes> input = 3;
  int input_size() const;
  private:
  int _internal_input_size() const;
  public:
  void clear_input();
  private:
  const ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >&
      _internal_input() const;
  ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >*
      _internal_mutable_input();
  public:
  const ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >&
      input() const;
  ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >*
      mutable_input();

  // string model_name = 1;
  void clear_model_name();
  const std::string& model_name() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_model_name(ArgT0&& arg0, ArgT... args);
  std::string* mutable_model_name();
  PROTOBUF_MUST_USE_RESULT std::string* release_model_name();
  void set_allocated_model_name(std::string* model_name);
  private:
  const std::string& _internal_model_name() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_model_name(const std::string& value);
  std::string* _internal_mutable_model_name();
  public:

  // string model_version = 2;
  void clear_model_version();
  const std::string& model_version() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_model_version(ArgT0&& arg0, ArgT... args);
  std::string* mutable_model_version();
  PROTOBUF_MUST_USE_RESULT std::string* release_model_version();
  void set_allocated_model_version(std::string* model_version);
  private:
  const std::string& _internal_model_version() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_model_version(const std::string& value);
  std::string* _internal_mutable_model_version();
  public:

  // @@protoc_insertion_point(class_scope:org.pytorch.serve.grpc.inference.PredictionsRequest)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::internal::MapField<
      PredictionsRequest_InputEntry_DoNotUse,
      std::string, std::string,
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_STRING,
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_BYTES> input_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr model_name_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr model_version_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_inference_2eproto;
};
// -------------------------------------------------------------------

class PredictionResponse final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:org.pytorch.serve.grpc.inference.PredictionResponse) */ {
 public:
  inline PredictionResponse() : PredictionResponse(nullptr) {}
  ~PredictionResponse() override;
  explicit constexpr PredictionResponse(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  PredictionResponse(const PredictionResponse& from);
  PredictionResponse(PredictionResponse&& from) noexcept
    : PredictionResponse() {
    *this = ::std::move(from);
  }

  inline PredictionResponse& operator=(const PredictionResponse& from) {
    CopyFrom(from);
    return *this;
  }
  inline PredictionResponse& operator=(PredictionResponse&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const PredictionResponse& default_instance() {
    return *internal_default_instance();
  }
  static inline const PredictionResponse* internal_default_instance() {
    return reinterpret_cast<const PredictionResponse*>(
               &_PredictionResponse_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    2;

  friend void swap(PredictionResponse& a, PredictionResponse& b) {
    a.Swap(&b);
  }
  inline void Swap(PredictionResponse* other) {
    if (other == this) return;
    if (GetOwningArena() == other->GetOwningArena()) {
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(PredictionResponse* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  inline PredictionResponse* New() const final {
    return new PredictionResponse();
  }

  PredictionResponse* New(::PROTOBUF_NAMESPACE_ID::Arena* arena) const final {
    return CreateMaybeMessage<PredictionResponse>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const PredictionResponse& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom(const PredictionResponse& from);
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to, const ::PROTOBUF_NAMESPACE_ID::Message& from);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  ::PROTOBUF_NAMESPACE_ID::uint8* _InternalSerialize(
      ::PROTOBUF_NAMESPACE_ID::uint8* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(PredictionResponse* other);
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "org.pytorch.serve.grpc.inference.PredictionResponse";
  }
  protected:
  explicit PredictionResponse(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kPredictionFieldNumber = 1,
  };
  // bytes prediction = 1;
  void clear_prediction();
  const std::string& prediction() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_prediction(ArgT0&& arg0, ArgT... args);
  std::string* mutable_prediction();
  PROTOBUF_MUST_USE_RESULT std::string* release_prediction();
  void set_allocated_prediction(std::string* prediction);
  private:
  const std::string& _internal_prediction() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_prediction(const std::string& value);
  std::string* _internal_mutable_prediction();
  public:

  // @@protoc_insertion_point(class_scope:org.pytorch.serve.grpc.inference.PredictionResponse)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr prediction_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_inference_2eproto;
};
// -------------------------------------------------------------------

class TorchServeHealthResponse final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:org.pytorch.serve.grpc.inference.TorchServeHealthResponse) */ {
 public:
  inline TorchServeHealthResponse() : TorchServeHealthResponse(nullptr) {}
  ~TorchServeHealthResponse() override;
  explicit constexpr TorchServeHealthResponse(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  TorchServeHealthResponse(const TorchServeHealthResponse& from);
  TorchServeHealthResponse(TorchServeHealthResponse&& from) noexcept
    : TorchServeHealthResponse() {
    *this = ::std::move(from);
  }

  inline TorchServeHealthResponse& operator=(const TorchServeHealthResponse& from) {
    CopyFrom(from);
    return *this;
  }
  inline TorchServeHealthResponse& operator=(TorchServeHealthResponse&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const TorchServeHealthResponse& default_instance() {
    return *internal_default_instance();
  }
  static inline const TorchServeHealthResponse* internal_default_instance() {
    return reinterpret_cast<const TorchServeHealthResponse*>(
               &_TorchServeHealthResponse_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    3;

  friend void swap(TorchServeHealthResponse& a, TorchServeHealthResponse& b) {
    a.Swap(&b);
  }
  inline void Swap(TorchServeHealthResponse* other) {
    if (other == this) return;
    if (GetOwningArena() == other->GetOwningArena()) {
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(TorchServeHealthResponse* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  inline TorchServeHealthResponse* New() const final {
    return new TorchServeHealthResponse();
  }

  TorchServeHealthResponse* New(::PROTOBUF_NAMESPACE_ID::Arena* arena) const final {
    return CreateMaybeMessage<TorchServeHealthResponse>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const TorchServeHealthResponse& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom(const TorchServeHealthResponse& from);
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to, const ::PROTOBUF_NAMESPACE_ID::Message& from);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  ::PROTOBUF_NAMESPACE_ID::uint8* _InternalSerialize(
      ::PROTOBUF_NAMESPACE_ID::uint8* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(TorchServeHealthResponse* other);
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "org.pytorch.serve.grpc.inference.TorchServeHealthResponse";
  }
  protected:
  explicit TorchServeHealthResponse(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kHealthFieldNumber = 1,
  };
  // string health = 1;
  void clear_health();
  const std::string& health() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_health(ArgT0&& arg0, ArgT... args);
  std::string* mutable_health();
  PROTOBUF_MUST_USE_RESULT std::string* release_health();
  void set_allocated_health(std::string* health);
  private:
  const std::string& _internal_health() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_health(const std::string& value);
  std::string* _internal_mutable_health();
  public:

  // @@protoc_insertion_point(class_scope:org.pytorch.serve.grpc.inference.TorchServeHealthResponse)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr health_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_inference_2eproto;
};
// ===================================================================


// ===================================================================

#ifdef __GNUC__
  #pragma GCC diagnostic push
  #pragma GCC diagnostic ignored "-Wstrict-aliasing"
#endif  // __GNUC__
// -------------------------------------------------------------------

// PredictionsRequest

// string model_name = 1;
inline void PredictionsRequest::clear_model_name() {
  model_name_.ClearToEmpty();
}
inline const std::string& PredictionsRequest::model_name() const {
  // @@protoc_insertion_point(field_get:org.pytorch.serve.grpc.inference.PredictionsRequest.model_name)
  return _internal_model_name();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void PredictionsRequest::set_model_name(ArgT0&& arg0, ArgT... args) {
 
 model_name_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:org.pytorch.serve.grpc.inference.PredictionsRequest.model_name)
}
inline std::string* PredictionsRequest::mutable_model_name() {
  std::string* _s = _internal_mutable_model_name();
  // @@protoc_insertion_point(field_mutable:org.pytorch.serve.grpc.inference.PredictionsRequest.model_name)
  return _s;
}
inline const std::string& PredictionsRequest::_internal_model_name() const {
  return model_name_.Get();
}
inline void PredictionsRequest::_internal_set_model_name(const std::string& value) {
  
  model_name_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* PredictionsRequest::_internal_mutable_model_name() {
  
  return model_name_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* PredictionsRequest::release_model_name() {
  // @@protoc_insertion_point(field_release:org.pytorch.serve.grpc.inference.PredictionsRequest.model_name)
  return model_name_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void PredictionsRequest::set_allocated_model_name(std::string* model_name) {
  if (model_name != nullptr) {
    
  } else {
    
  }
  model_name_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), model_name,
      GetArenaForAllocation());
  // @@protoc_insertion_point(field_set_allocated:org.pytorch.serve.grpc.inference.PredictionsRequest.model_name)
}

// string model_version = 2;
inline void PredictionsRequest::clear_model_version() {
  model_version_.ClearToEmpty();
}
inline const std::string& PredictionsRequest::model_version() const {
  // @@protoc_insertion_point(field_get:org.pytorch.serve.grpc.inference.PredictionsRequest.model_version)
  return _internal_model_version();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void PredictionsRequest::set_model_version(ArgT0&& arg0, ArgT... args) {
 
 model_version_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:org.pytorch.serve.grpc.inference.PredictionsRequest.model_version)
}
inline std::string* PredictionsRequest::mutable_model_version() {
  std::string* _s = _internal_mutable_model_version();
  // @@protoc_insertion_point(field_mutable:org.pytorch.serve.grpc.inference.PredictionsRequest.model_version)
  return _s;
}
inline const std::string& PredictionsRequest::_internal_model_version() const {
  return model_version_.Get();
}
inline void PredictionsRequest::_internal_set_model_version(const std::string& value) {
  
  model_version_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* PredictionsRequest::_internal_mutable_model_version() {
  
  return model_version_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* PredictionsRequest::release_model_version() {
  // @@protoc_insertion_point(field_release:org.pytorch.serve.grpc.inference.PredictionsRequest.model_version)
  return model_version_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void PredictionsRequest::set_allocated_model_version(std::string* model_version) {
  if (model_version != nullptr) {
    
  } else {
    
  }
  model_version_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), model_version,
      GetArenaForAllocation());
  // @@protoc_insertion_point(field_set_allocated:org.pytorch.serve.grpc.inference.PredictionsRequest.model_version)
}

// map<string, bytes> input = 3;
inline int PredictionsRequest::_internal_input_size() const {
  return input_.size();
}
inline int PredictionsRequest::input_size() const {
  return _internal_input_size();
}
inline void PredictionsRequest::clear_input() {
  input_.Clear();
}
inline const ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >&
PredictionsRequest::_internal_input() const {
  return input_.GetMap();
}
inline const ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >&
PredictionsRequest::input() const {
  // @@protoc_insertion_point(field_map:org.pytorch.serve.grpc.inference.PredictionsRequest.input)
  return _internal_input();
}
inline ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >*
PredictionsRequest::_internal_mutable_input() {
  return input_.MutableMap();
}
inline ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >*
PredictionsRequest::mutable_input() {
  // @@protoc_insertion_point(field_mutable_map:org.pytorch.serve.grpc.inference.PredictionsRequest.input)
  return _internal_mutable_input();
}

// -------------------------------------------------------------------

// PredictionResponse

// bytes prediction = 1;
inline void PredictionResponse::clear_prediction() {
  prediction_.ClearToEmpty();
}
inline const std::string& PredictionResponse::prediction() const {
  // @@protoc_insertion_point(field_get:org.pytorch.serve.grpc.inference.PredictionResponse.prediction)
  return _internal_prediction();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void PredictionResponse::set_prediction(ArgT0&& arg0, ArgT... args) {
 
 prediction_.SetBytes(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:org.pytorch.serve.grpc.inference.PredictionResponse.prediction)
}
inline std::string* PredictionResponse::mutable_prediction() {
  std::string* _s = _internal_mutable_prediction();
  // @@protoc_insertion_point(field_mutable:org.pytorch.serve.grpc.inference.PredictionResponse.prediction)
  return _s;
}
inline const std::string& PredictionResponse::_internal_prediction() const {
  return prediction_.Get();
}
inline void PredictionResponse::_internal_set_prediction(const std::string& value) {
  
  prediction_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* PredictionResponse::_internal_mutable_prediction() {
  
  return prediction_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* PredictionResponse::release_prediction() {
  // @@protoc_insertion_point(field_release:org.pytorch.serve.grpc.inference.PredictionResponse.prediction)
  return prediction_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void PredictionResponse::set_allocated_prediction(std::string* prediction) {
  if (prediction != nullptr) {
    
  } else {
    
  }
  prediction_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), prediction,
      GetArenaForAllocation());
  // @@protoc_insertion_point(field_set_allocated:org.pytorch.serve.grpc.inference.PredictionResponse.prediction)
}

// -------------------------------------------------------------------

// TorchServeHealthResponse

// string health = 1;
inline void TorchServeHealthResponse::clear_health() {
  health_.ClearToEmpty();
}
inline const std::string& TorchServeHealthResponse::health() const {
  // @@protoc_insertion_point(field_get:org.pytorch.serve.grpc.inference.TorchServeHealthResponse.health)
  return _internal_health();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void TorchServeHealthResponse::set_health(ArgT0&& arg0, ArgT... args) {
 
 health_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:org.pytorch.serve.grpc.inference.TorchServeHealthResponse.health)
}
inline std::string* TorchServeHealthResponse::mutable_health() {
  std::string* _s = _internal_mutable_health();
  // @@protoc_insertion_point(field_mutable:org.pytorch.serve.grpc.inference.TorchServeHealthResponse.health)
  return _s;
}
inline const std::string& TorchServeHealthResponse::_internal_health() const {
  return health_.Get();
}
inline void TorchServeHealthResponse::_internal_set_health(const std::string& value) {
  
  health_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* TorchServeHealthResponse::_internal_mutable_health() {
  
  return health_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* TorchServeHealthResponse::release_health() {
  // @@protoc_insertion_point(field_release:org.pytorch.serve.grpc.inference.TorchServeHealthResponse.health)
  return health_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void TorchServeHealthResponse::set_allocated_health(std::string* health) {
  if (health != nullptr) {
    
  } else {
    
  }
  health_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), health,
      GetArenaForAllocation());
  // @@protoc_insertion_point(field_set_allocated:org.pytorch.serve.grpc.inference.TorchServeHealthResponse.health)
}

#ifdef __GNUC__
  #pragma GCC diagnostic pop
#endif  // __GNUC__
// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------


// @@protoc_insertion_point(namespace_scope)

}  // namespace inference
}  // namespace grpc
}  // namespace serve
}  // namespace pytorch
}  // namespace org

// @@protoc_insertion_point(global_scope)

#include <google/protobuf/port_undef.inc>
#endif  // GOOGLE_PROTOBUF_INCLUDED_GOOGLE_PROTOBUF_INCLUDED_inference_2eproto
