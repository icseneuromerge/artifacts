// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: inference.proto

#include "inference.pb.h"

#include <algorithm>

#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/extension_set.h>
#include <google/protobuf/wire_format_lite.h>
#include <google/protobuf/descriptor.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/reflection_ops.h>
#include <google/protobuf/wire_format.h>
// @@protoc_insertion_point(includes)
#include <google/protobuf/port_def.inc>

PROTOBUF_PRAGMA_INIT_SEG
namespace org {
namespace pytorch {
namespace serve {
namespace grpc {
namespace inference {
constexpr PredictionsRequest_InputEntry_DoNotUse::PredictionsRequest_InputEntry_DoNotUse(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized){}
struct PredictionsRequest_InputEntry_DoNotUseDefaultTypeInternal {
  constexpr PredictionsRequest_InputEntry_DoNotUseDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~PredictionsRequest_InputEntry_DoNotUseDefaultTypeInternal() {}
  union {
    PredictionsRequest_InputEntry_DoNotUse _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PredictionsRequest_InputEntry_DoNotUseDefaultTypeInternal _PredictionsRequest_InputEntry_DoNotUse_default_instance_;
constexpr PredictionsRequest::PredictionsRequest(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized)
  : input_(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{})
  , model_name_(&::PROTOBUF_NAMESPACE_ID::internal::fixed_address_empty_string)
  , model_version_(&::PROTOBUF_NAMESPACE_ID::internal::fixed_address_empty_string){}
struct PredictionsRequestDefaultTypeInternal {
  constexpr PredictionsRequestDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~PredictionsRequestDefaultTypeInternal() {}
  union {
    PredictionsRequest _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PredictionsRequestDefaultTypeInternal _PredictionsRequest_default_instance_;
constexpr PredictionResponse::PredictionResponse(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized)
  : prediction_(&::PROTOBUF_NAMESPACE_ID::internal::fixed_address_empty_string){}
struct PredictionResponseDefaultTypeInternal {
  constexpr PredictionResponseDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~PredictionResponseDefaultTypeInternal() {}
  union {
    PredictionResponse _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PredictionResponseDefaultTypeInternal _PredictionResponse_default_instance_;
constexpr TorchServeHealthResponse::TorchServeHealthResponse(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized)
  : health_(&::PROTOBUF_NAMESPACE_ID::internal::fixed_address_empty_string){}
struct TorchServeHealthResponseDefaultTypeInternal {
  constexpr TorchServeHealthResponseDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~TorchServeHealthResponseDefaultTypeInternal() {}
  union {
    TorchServeHealthResponse _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT TorchServeHealthResponseDefaultTypeInternal _TorchServeHealthResponse_default_instance_;
}  // namespace inference
}  // namespace grpc
}  // namespace serve
}  // namespace pytorch
}  // namespace org
static ::PROTOBUF_NAMESPACE_ID::Metadata file_level_metadata_inference_2eproto[4];
static constexpr ::PROTOBUF_NAMESPACE_ID::EnumDescriptor const** file_level_enum_descriptors_inference_2eproto = nullptr;
static constexpr ::PROTOBUF_NAMESPACE_ID::ServiceDescriptor const** file_level_service_descriptors_inference_2eproto = nullptr;

const ::PROTOBUF_NAMESPACE_ID::uint32 TableStruct_inference_2eproto::offsets[] PROTOBUF_SECTION_VARIABLE(protodesc_cold) = {
  PROTOBUF_FIELD_OFFSET(::org::pytorch::serve::grpc::inference::PredictionsRequest_InputEntry_DoNotUse, _has_bits_),
  PROTOBUF_FIELD_OFFSET(::org::pytorch::serve::grpc::inference::PredictionsRequest_InputEntry_DoNotUse, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::org::pytorch::serve::grpc::inference::PredictionsRequest_InputEntry_DoNotUse, key_),
  PROTOBUF_FIELD_OFFSET(::org::pytorch::serve::grpc::inference::PredictionsRequest_InputEntry_DoNotUse, value_),
  0,
  1,
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::org::pytorch::serve::grpc::inference::PredictionsRequest, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::org::pytorch::serve::grpc::inference::PredictionsRequest, model_name_),
  PROTOBUF_FIELD_OFFSET(::org::pytorch::serve::grpc::inference::PredictionsRequest, model_version_),
  PROTOBUF_FIELD_OFFSET(::org::pytorch::serve::grpc::inference::PredictionsRequest, input_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::org::pytorch::serve::grpc::inference::PredictionResponse, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::org::pytorch::serve::grpc::inference::PredictionResponse, prediction_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::org::pytorch::serve::grpc::inference::TorchServeHealthResponse, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::org::pytorch::serve::grpc::inference::TorchServeHealthResponse, health_),
};
static const ::PROTOBUF_NAMESPACE_ID::internal::MigrationSchema schemas[] PROTOBUF_SECTION_VARIABLE(protodesc_cold) = {
  { 0, 8, -1, sizeof(::org::pytorch::serve::grpc::inference::PredictionsRequest_InputEntry_DoNotUse)},
  { 10, -1, -1, sizeof(::org::pytorch::serve::grpc::inference::PredictionsRequest)},
  { 19, -1, -1, sizeof(::org::pytorch::serve::grpc::inference::PredictionResponse)},
  { 26, -1, -1, sizeof(::org::pytorch::serve::grpc::inference::TorchServeHealthResponse)},
};

static ::PROTOBUF_NAMESPACE_ID::Message const * const file_default_instances[] = {
  reinterpret_cast<const ::PROTOBUF_NAMESPACE_ID::Message*>(&::org::pytorch::serve::grpc::inference::_PredictionsRequest_InputEntry_DoNotUse_default_instance_),
  reinterpret_cast<const ::PROTOBUF_NAMESPACE_ID::Message*>(&::org::pytorch::serve::grpc::inference::_PredictionsRequest_default_instance_),
  reinterpret_cast<const ::PROTOBUF_NAMESPACE_ID::Message*>(&::org::pytorch::serve::grpc::inference::_PredictionResponse_default_instance_),
  reinterpret_cast<const ::PROTOBUF_NAMESPACE_ID::Message*>(&::org::pytorch::serve::grpc::inference::_TorchServeHealthResponse_default_instance_),
};

const char descriptor_table_protodef_inference_2eproto[] PROTOBUF_SECTION_VARIABLE(protodesc_cold) =
  "\n\017inference.proto\022 org.pytorch.serve.grp"
  "c.inference\032\033google/protobuf/empty.proto"
  "\"\275\001\n\022PredictionsRequest\022\022\n\nmodel_name\030\001 "
  "\001(\t\022\025\n\rmodel_version\030\002 \001(\t\022N\n\005input\030\003 \003("
  "\0132\?.org.pytorch.serve.grpc.inference.Pre"
  "dictionsRequest.InputEntry\032,\n\nInputEntry"
  "\022\013\n\003key\030\001 \001(\t\022\r\n\005value\030\002 \001(\014:\0028\001\"(\n\022Pred"
  "ictionResponse\022\022\n\nprediction\030\001 \001(\014\"*\n\030To"
  "rchServeHealthResponse\022\016\n\006health\030\001 \001(\t2\361"
  "\001\n\024InferenceAPIsService\022\\\n\004Ping\022\026.google"
  ".protobuf.Empty\032:.org.pytorch.serve.grpc"
  ".inference.TorchServeHealthResponse\"\000\022{\n"
  "\013Predictions\0224.org.pytorch.serve.grpc.in"
  "ference.PredictionsRequest\0324.org.pytorch"
  ".serve.grpc.inference.PredictionResponse"
  "\"\000B\002P\001b\006proto3"
  ;
static const ::PROTOBUF_NAMESPACE_ID::internal::DescriptorTable*const descriptor_table_inference_2eproto_deps[1] = {
  &::descriptor_table_google_2fprotobuf_2fempty_2eproto,
};
static ::PROTOBUF_NAMESPACE_ID::internal::once_flag descriptor_table_inference_2eproto_once;
const ::PROTOBUF_NAMESPACE_ID::internal::DescriptorTable descriptor_table_inference_2eproto = {
  false, false, 614, descriptor_table_protodef_inference_2eproto, "inference.proto", 
  &descriptor_table_inference_2eproto_once, descriptor_table_inference_2eproto_deps, 1, 4,
  schemas, file_default_instances, TableStruct_inference_2eproto::offsets,
  file_level_metadata_inference_2eproto, file_level_enum_descriptors_inference_2eproto, file_level_service_descriptors_inference_2eproto,
};
PROTOBUF_ATTRIBUTE_WEAK const ::PROTOBUF_NAMESPACE_ID::internal::DescriptorTable* descriptor_table_inference_2eproto_getter() {
  return &descriptor_table_inference_2eproto;
}

// Force running AddDescriptors() at dynamic initialization time.
PROTOBUF_ATTRIBUTE_INIT_PRIORITY static ::PROTOBUF_NAMESPACE_ID::internal::AddDescriptorsRunner dynamic_init_dummy_inference_2eproto(&descriptor_table_inference_2eproto);
namespace org {
namespace pytorch {
namespace serve {
namespace grpc {
namespace inference {

// ===================================================================

PredictionsRequest_InputEntry_DoNotUse::PredictionsRequest_InputEntry_DoNotUse() {}
PredictionsRequest_InputEntry_DoNotUse::PredictionsRequest_InputEntry_DoNotUse(::PROTOBUF_NAMESPACE_ID::Arena* arena)
    : SuperType(arena) {}
void PredictionsRequest_InputEntry_DoNotUse::MergeFrom(const PredictionsRequest_InputEntry_DoNotUse& other) {
  MergeFromInternal(other);
}
::PROTOBUF_NAMESPACE_ID::Metadata PredictionsRequest_InputEntry_DoNotUse::GetMetadata() const {
  return ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(
      &descriptor_table_inference_2eproto_getter, &descriptor_table_inference_2eproto_once,
      file_level_metadata_inference_2eproto[0]);
}

// ===================================================================

class PredictionsRequest::_Internal {
 public:
};

PredictionsRequest::PredictionsRequest(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned),
  input_(arena) {
  SharedCtor();
  if (!is_message_owned) {
    RegisterArenaDtor(arena);
  }
  // @@protoc_insertion_point(arena_constructor:org.pytorch.serve.grpc.inference.PredictionsRequest)
}
PredictionsRequest::PredictionsRequest(const PredictionsRequest& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  input_.MergeFrom(from.input_);
  model_name_.UnsafeSetDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
  if (!from._internal_model_name().empty()) {
    model_name_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, from._internal_model_name(), 
      GetArenaForAllocation());
  }
  model_version_.UnsafeSetDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
  if (!from._internal_model_version().empty()) {
    model_version_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, from._internal_model_version(), 
      GetArenaForAllocation());
  }
  // @@protoc_insertion_point(copy_constructor:org.pytorch.serve.grpc.inference.PredictionsRequest)
}

void PredictionsRequest::SharedCtor() {
model_name_.UnsafeSetDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
model_version_.UnsafeSetDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
}

PredictionsRequest::~PredictionsRequest() {
  // @@protoc_insertion_point(destructor:org.pytorch.serve.grpc.inference.PredictionsRequest)
  if (GetArenaForAllocation() != nullptr) return;
  SharedDtor();
  _internal_metadata_.Delete<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

inline void PredictionsRequest::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  model_name_.DestroyNoArena(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
  model_version_.DestroyNoArena(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
}

void PredictionsRequest::ArenaDtor(void* object) {
  PredictionsRequest* _this = reinterpret_cast< PredictionsRequest* >(object);
  (void)_this;
  _this->input_. ~MapField();
}
inline void PredictionsRequest::RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena) {
  if (arena != nullptr) {
    arena->OwnCustomDestructor(this, &PredictionsRequest::ArenaDtor);
  }
}
void PredictionsRequest::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}

void PredictionsRequest::Clear() {
// @@protoc_insertion_point(message_clear_start:org.pytorch.serve.grpc.inference.PredictionsRequest)
  ::PROTOBUF_NAMESPACE_ID::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  input_.Clear();
  model_name_.ClearToEmpty();
  model_version_.ClearToEmpty();
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* PredictionsRequest::_InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    ::PROTOBUF_NAMESPACE_ID::uint32 tag;
    ptr = ::PROTOBUF_NAMESPACE_ID::internal::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // string model_name = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<::PROTOBUF_NAMESPACE_ID::uint8>(tag) == 10)) {
          auto str = _internal_mutable_model_name();
          ptr = ::PROTOBUF_NAMESPACE_ID::internal::InlineGreedyStringParser(str, ptr, ctx);
          CHK_(::PROTOBUF_NAMESPACE_ID::internal::VerifyUTF8(str, "org.pytorch.serve.grpc.inference.PredictionsRequest.model_name"));
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // string model_version = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<::PROTOBUF_NAMESPACE_ID::uint8>(tag) == 18)) {
          auto str = _internal_mutable_model_version();
          ptr = ::PROTOBUF_NAMESPACE_ID::internal::InlineGreedyStringParser(str, ptr, ctx);
          CHK_(::PROTOBUF_NAMESPACE_ID::internal::VerifyUTF8(str, "org.pytorch.serve.grpc.inference.PredictionsRequest.model_version"));
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // map<string, bytes> input = 3;
      case 3:
        if (PROTOBUF_PREDICT_TRUE(static_cast<::PROTOBUF_NAMESPACE_ID::uint8>(tag) == 26)) {
          ptr -= 1;
          do {
            ptr += 1;
            ptr = ctx->ParseMessage(&input_, ptr);
            CHK_(ptr);
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<26>(ptr));
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

::PROTOBUF_NAMESPACE_ID::uint8* PredictionsRequest::_InternalSerialize(
    ::PROTOBUF_NAMESPACE_ID::uint8* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:org.pytorch.serve.grpc.inference.PredictionsRequest)
  ::PROTOBUF_NAMESPACE_ID::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // string model_name = 1;
  if (!this->_internal_model_name().empty()) {
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
      this->_internal_model_name().data(), static_cast<int>(this->_internal_model_name().length()),
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
      "org.pytorch.serve.grpc.inference.PredictionsRequest.model_name");
    target = stream->WriteStringMaybeAliased(
        1, this->_internal_model_name(), target);
  }

  // string model_version = 2;
  if (!this->_internal_model_version().empty()) {
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
      this->_internal_model_version().data(), static_cast<int>(this->_internal_model_version().length()),
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
      "org.pytorch.serve.grpc.inference.PredictionsRequest.model_version");
    target = stream->WriteStringMaybeAliased(
        2, this->_internal_model_version(), target);
  }

  // map<string, bytes> input = 3;
  if (!this->_internal_input().empty()) {
    typedef ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >::const_pointer
        ConstPtr;
    typedef ConstPtr SortItem;
    typedef ::PROTOBUF_NAMESPACE_ID::internal::CompareByDerefFirst<SortItem> Less;
    struct Utf8Check {
      static void Check(ConstPtr p) {
        (void)p;
        ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
          p->first.data(), static_cast<int>(p->first.length()),
          ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
          "org.pytorch.serve.grpc.inference.PredictionsRequest.InputEntry.key");
      }
    };

    if (stream->IsSerializationDeterministic() &&
        this->_internal_input().size() > 1) {
      ::std::unique_ptr<SortItem[]> items(
          new SortItem[this->_internal_input().size()]);
      typedef ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >::size_type size_type;
      size_type n = 0;
      for (::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >::const_iterator
          it = this->_internal_input().begin();
          it != this->_internal_input().end(); ++it, ++n) {
        items[static_cast<ptrdiff_t>(n)] = SortItem(&*it);
      }
      ::std::sort(&items[0], &items[static_cast<ptrdiff_t>(n)], Less());
      for (size_type i = 0; i < n; i++) {
        target = PredictionsRequest_InputEntry_DoNotUse::Funcs::InternalSerialize(3, items[static_cast<ptrdiff_t>(i)]->first, items[static_cast<ptrdiff_t>(i)]->second, target, stream);
        Utf8Check::Check(&(*items[static_cast<ptrdiff_t>(i)]));
      }
    } else {
      for (::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >::const_iterator
          it = this->_internal_input().begin();
          it != this->_internal_input().end(); ++it) {
        target = PredictionsRequest_InputEntry_DoNotUse::Funcs::InternalSerialize(3, it->first, it->second, target, stream);
        Utf8Check::Check(&(*it));
      }
    }
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:org.pytorch.serve.grpc.inference.PredictionsRequest)
  return target;
}

size_t PredictionsRequest::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:org.pytorch.serve.grpc.inference.PredictionsRequest)
  size_t total_size = 0;

  ::PROTOBUF_NAMESPACE_ID::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // map<string, bytes> input = 3;
  total_size += 1 *
      ::PROTOBUF_NAMESPACE_ID::internal::FromIntSize(this->_internal_input_size());
  for (::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >::const_iterator
      it = this->_internal_input().begin();
      it != this->_internal_input().end(); ++it) {
    total_size += PredictionsRequest_InputEntry_DoNotUse::Funcs::ByteSizeLong(it->first, it->second);
  }

  // string model_name = 1;
  if (!this->_internal_model_name().empty()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::StringSize(
        this->_internal_model_name());
  }

  // string model_version = 2;
  if (!this->_internal_model_version().empty()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::StringSize(
        this->_internal_model_version());
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData PredictionsRequest::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSizeCheck,
    PredictionsRequest::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*PredictionsRequest::GetClassData() const { return &_class_data_; }

void PredictionsRequest::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to,
                      const ::PROTOBUF_NAMESPACE_ID::Message& from) {
  static_cast<PredictionsRequest *>(to)->MergeFrom(
      static_cast<const PredictionsRequest &>(from));
}


void PredictionsRequest::MergeFrom(const PredictionsRequest& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:org.pytorch.serve.grpc.inference.PredictionsRequest)
  GOOGLE_DCHECK_NE(&from, this);
  ::PROTOBUF_NAMESPACE_ID::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  input_.MergeFrom(from.input_);
  if (!from._internal_model_name().empty()) {
    _internal_set_model_name(from._internal_model_name());
  }
  if (!from._internal_model_version().empty()) {
    _internal_set_model_version(from._internal_model_version());
  }
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void PredictionsRequest::CopyFrom(const PredictionsRequest& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:org.pytorch.serve.grpc.inference.PredictionsRequest)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PredictionsRequest::IsInitialized() const {
  return true;
}

void PredictionsRequest::InternalSwap(PredictionsRequest* other) {
  using std::swap;
  auto* lhs_arena = GetArenaForAllocation();
  auto* rhs_arena = other->GetArenaForAllocation();
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  input_.InternalSwap(&other->input_);
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::InternalSwap(
      &::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(),
      &model_name_, lhs_arena,
      &other->model_name_, rhs_arena
  );
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::InternalSwap(
      &::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(),
      &model_version_, lhs_arena,
      &other->model_version_, rhs_arena
  );
}

::PROTOBUF_NAMESPACE_ID::Metadata PredictionsRequest::GetMetadata() const {
  return ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(
      &descriptor_table_inference_2eproto_getter, &descriptor_table_inference_2eproto_once,
      file_level_metadata_inference_2eproto[1]);
}

// ===================================================================

class PredictionResponse::_Internal {
 public:
};

PredictionResponse::PredictionResponse(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor();
  if (!is_message_owned) {
    RegisterArenaDtor(arena);
  }
  // @@protoc_insertion_point(arena_constructor:org.pytorch.serve.grpc.inference.PredictionResponse)
}
PredictionResponse::PredictionResponse(const PredictionResponse& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  prediction_.UnsafeSetDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
  if (!from._internal_prediction().empty()) {
    prediction_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, from._internal_prediction(), 
      GetArenaForAllocation());
  }
  // @@protoc_insertion_point(copy_constructor:org.pytorch.serve.grpc.inference.PredictionResponse)
}

void PredictionResponse::SharedCtor() {
prediction_.UnsafeSetDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
}

PredictionResponse::~PredictionResponse() {
  // @@protoc_insertion_point(destructor:org.pytorch.serve.grpc.inference.PredictionResponse)
  if (GetArenaForAllocation() != nullptr) return;
  SharedDtor();
  _internal_metadata_.Delete<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

inline void PredictionResponse::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  prediction_.DestroyNoArena(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
}

void PredictionResponse::ArenaDtor(void* object) {
  PredictionResponse* _this = reinterpret_cast< PredictionResponse* >(object);
  (void)_this;
}
void PredictionResponse::RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena*) {
}
void PredictionResponse::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}

void PredictionResponse::Clear() {
// @@protoc_insertion_point(message_clear_start:org.pytorch.serve.grpc.inference.PredictionResponse)
  ::PROTOBUF_NAMESPACE_ID::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  prediction_.ClearToEmpty();
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* PredictionResponse::_InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    ::PROTOBUF_NAMESPACE_ID::uint32 tag;
    ptr = ::PROTOBUF_NAMESPACE_ID::internal::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // bytes prediction = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<::PROTOBUF_NAMESPACE_ID::uint8>(tag) == 10)) {
          auto str = _internal_mutable_prediction();
          ptr = ::PROTOBUF_NAMESPACE_ID::internal::InlineGreedyStringParser(str, ptr, ctx);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

::PROTOBUF_NAMESPACE_ID::uint8* PredictionResponse::_InternalSerialize(
    ::PROTOBUF_NAMESPACE_ID::uint8* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:org.pytorch.serve.grpc.inference.PredictionResponse)
  ::PROTOBUF_NAMESPACE_ID::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // bytes prediction = 1;
  if (!this->_internal_prediction().empty()) {
    target = stream->WriteBytesMaybeAliased(
        1, this->_internal_prediction(), target);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:org.pytorch.serve.grpc.inference.PredictionResponse)
  return target;
}

size_t PredictionResponse::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:org.pytorch.serve.grpc.inference.PredictionResponse)
  size_t total_size = 0;

  ::PROTOBUF_NAMESPACE_ID::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // bytes prediction = 1;
  if (!this->_internal_prediction().empty()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::BytesSize(
        this->_internal_prediction());
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData PredictionResponse::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSizeCheck,
    PredictionResponse::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*PredictionResponse::GetClassData() const { return &_class_data_; }

void PredictionResponse::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to,
                      const ::PROTOBUF_NAMESPACE_ID::Message& from) {
  static_cast<PredictionResponse *>(to)->MergeFrom(
      static_cast<const PredictionResponse &>(from));
}


void PredictionResponse::MergeFrom(const PredictionResponse& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:org.pytorch.serve.grpc.inference.PredictionResponse)
  GOOGLE_DCHECK_NE(&from, this);
  ::PROTOBUF_NAMESPACE_ID::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (!from._internal_prediction().empty()) {
    _internal_set_prediction(from._internal_prediction());
  }
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void PredictionResponse::CopyFrom(const PredictionResponse& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:org.pytorch.serve.grpc.inference.PredictionResponse)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool PredictionResponse::IsInitialized() const {
  return true;
}

void PredictionResponse::InternalSwap(PredictionResponse* other) {
  using std::swap;
  auto* lhs_arena = GetArenaForAllocation();
  auto* rhs_arena = other->GetArenaForAllocation();
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::InternalSwap(
      &::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(),
      &prediction_, lhs_arena,
      &other->prediction_, rhs_arena
  );
}

::PROTOBUF_NAMESPACE_ID::Metadata PredictionResponse::GetMetadata() const {
  return ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(
      &descriptor_table_inference_2eproto_getter, &descriptor_table_inference_2eproto_once,
      file_level_metadata_inference_2eproto[2]);
}

// ===================================================================

class TorchServeHealthResponse::_Internal {
 public:
};

TorchServeHealthResponse::TorchServeHealthResponse(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor();
  if (!is_message_owned) {
    RegisterArenaDtor(arena);
  }
  // @@protoc_insertion_point(arena_constructor:org.pytorch.serve.grpc.inference.TorchServeHealthResponse)
}
TorchServeHealthResponse::TorchServeHealthResponse(const TorchServeHealthResponse& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  health_.UnsafeSetDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
  if (!from._internal_health().empty()) {
    health_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, from._internal_health(), 
      GetArenaForAllocation());
  }
  // @@protoc_insertion_point(copy_constructor:org.pytorch.serve.grpc.inference.TorchServeHealthResponse)
}

void TorchServeHealthResponse::SharedCtor() {
health_.UnsafeSetDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
}

TorchServeHealthResponse::~TorchServeHealthResponse() {
  // @@protoc_insertion_point(destructor:org.pytorch.serve.grpc.inference.TorchServeHealthResponse)
  if (GetArenaForAllocation() != nullptr) return;
  SharedDtor();
  _internal_metadata_.Delete<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

inline void TorchServeHealthResponse::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  health_.DestroyNoArena(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
}

void TorchServeHealthResponse::ArenaDtor(void* object) {
  TorchServeHealthResponse* _this = reinterpret_cast< TorchServeHealthResponse* >(object);
  (void)_this;
}
void TorchServeHealthResponse::RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena*) {
}
void TorchServeHealthResponse::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}

void TorchServeHealthResponse::Clear() {
// @@protoc_insertion_point(message_clear_start:org.pytorch.serve.grpc.inference.TorchServeHealthResponse)
  ::PROTOBUF_NAMESPACE_ID::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  health_.ClearToEmpty();
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* TorchServeHealthResponse::_InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    ::PROTOBUF_NAMESPACE_ID::uint32 tag;
    ptr = ::PROTOBUF_NAMESPACE_ID::internal::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // string health = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<::PROTOBUF_NAMESPACE_ID::uint8>(tag) == 10)) {
          auto str = _internal_mutable_health();
          ptr = ::PROTOBUF_NAMESPACE_ID::internal::InlineGreedyStringParser(str, ptr, ctx);
          CHK_(::PROTOBUF_NAMESPACE_ID::internal::VerifyUTF8(str, "org.pytorch.serve.grpc.inference.TorchServeHealthResponse.health"));
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

::PROTOBUF_NAMESPACE_ID::uint8* TorchServeHealthResponse::_InternalSerialize(
    ::PROTOBUF_NAMESPACE_ID::uint8* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:org.pytorch.serve.grpc.inference.TorchServeHealthResponse)
  ::PROTOBUF_NAMESPACE_ID::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // string health = 1;
  if (!this->_internal_health().empty()) {
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
      this->_internal_health().data(), static_cast<int>(this->_internal_health().length()),
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
      "org.pytorch.serve.grpc.inference.TorchServeHealthResponse.health");
    target = stream->WriteStringMaybeAliased(
        1, this->_internal_health(), target);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:org.pytorch.serve.grpc.inference.TorchServeHealthResponse)
  return target;
}

size_t TorchServeHealthResponse::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:org.pytorch.serve.grpc.inference.TorchServeHealthResponse)
  size_t total_size = 0;

  ::PROTOBUF_NAMESPACE_ID::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // string health = 1;
  if (!this->_internal_health().empty()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::StringSize(
        this->_internal_health());
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData TorchServeHealthResponse::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSizeCheck,
    TorchServeHealthResponse::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*TorchServeHealthResponse::GetClassData() const { return &_class_data_; }

void TorchServeHealthResponse::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to,
                      const ::PROTOBUF_NAMESPACE_ID::Message& from) {
  static_cast<TorchServeHealthResponse *>(to)->MergeFrom(
      static_cast<const TorchServeHealthResponse &>(from));
}


void TorchServeHealthResponse::MergeFrom(const TorchServeHealthResponse& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:org.pytorch.serve.grpc.inference.TorchServeHealthResponse)
  GOOGLE_DCHECK_NE(&from, this);
  ::PROTOBUF_NAMESPACE_ID::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (!from._internal_health().empty()) {
    _internal_set_health(from._internal_health());
  }
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void TorchServeHealthResponse::CopyFrom(const TorchServeHealthResponse& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:org.pytorch.serve.grpc.inference.TorchServeHealthResponse)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool TorchServeHealthResponse::IsInitialized() const {
  return true;
}

void TorchServeHealthResponse::InternalSwap(TorchServeHealthResponse* other) {
  using std::swap;
  auto* lhs_arena = GetArenaForAllocation();
  auto* rhs_arena = other->GetArenaForAllocation();
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::InternalSwap(
      &::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(),
      &health_, lhs_arena,
      &other->health_, rhs_arena
  );
}

::PROTOBUF_NAMESPACE_ID::Metadata TorchServeHealthResponse::GetMetadata() const {
  return ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(
      &descriptor_table_inference_2eproto_getter, &descriptor_table_inference_2eproto_once,
      file_level_metadata_inference_2eproto[3]);
}

// @@protoc_insertion_point(namespace_scope)
}  // namespace inference
}  // namespace grpc
}  // namespace serve
}  // namespace pytorch
}  // namespace org
PROTOBUF_NAMESPACE_OPEN
template<> PROTOBUF_NOINLINE ::org::pytorch::serve::grpc::inference::PredictionsRequest_InputEntry_DoNotUse* Arena::CreateMaybeMessage< ::org::pytorch::serve::grpc::inference::PredictionsRequest_InputEntry_DoNotUse >(Arena* arena) {
  return Arena::CreateMessageInternal< ::org::pytorch::serve::grpc::inference::PredictionsRequest_InputEntry_DoNotUse >(arena);
}
template<> PROTOBUF_NOINLINE ::org::pytorch::serve::grpc::inference::PredictionsRequest* Arena::CreateMaybeMessage< ::org::pytorch::serve::grpc::inference::PredictionsRequest >(Arena* arena) {
  return Arena::CreateMessageInternal< ::org::pytorch::serve::grpc::inference::PredictionsRequest >(arena);
}
template<> PROTOBUF_NOINLINE ::org::pytorch::serve::grpc::inference::PredictionResponse* Arena::CreateMaybeMessage< ::org::pytorch::serve::grpc::inference::PredictionResponse >(Arena* arena) {
  return Arena::CreateMessageInternal< ::org::pytorch::serve::grpc::inference::PredictionResponse >(arena);
}
template<> PROTOBUF_NOINLINE ::org::pytorch::serve::grpc::inference::TorchServeHealthResponse* Arena::CreateMaybeMessage< ::org::pytorch::serve::grpc::inference::TorchServeHealthResponse >(Arena* arena) {
  return Arena::CreateMessageInternal< ::org::pytorch::serve::grpc::inference::TorchServeHealthResponse >(arena);
}
PROTOBUF_NAMESPACE_CLOSE

// @@protoc_insertion_point(global_scope)
#include <google/protobuf/port_undef.inc>
